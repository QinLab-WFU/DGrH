import torch
from torch import nn

from _utils import mean_average_precision


def sigmoid(tensor, temp=1.0):
    """temperature controlled sigmoid
    takes as input a torch tensor (tensor) and passes it through a sigmoid, controlled by temperature: temp
    """
    exponent = -tensor / temp
    # clamp the input tensor for stability
    exponent = torch.clamp(exponent, min=-50, max=50)
    y = 1.0 / (1.0 + torch.exp(exponent))
    return y


class SmoothAP(nn.Module):
    """PyTorch's implementation of the Smooth-AP loss.
    implementation of the Smooth-AP loss. Takes as input the mini-batch of CNN-produced feature embeddings and returns
    the value of the Smooth-AP loss. The mini-batch must be formed of a defined number of classes. Each class must
    have the same number of instances represented in the mini-batch and must be ordered sequentially by class.
    e.g. the labels for a mini-batch with batch size 9, and 3 represented classes (A,B,C) must look like:
        labels = ( A, A, A, B, B, B, C, C, C)
    (the order of the classes however does not matter)
    For each instance in the mini-batch, the loss computes the Smooth-AP when it is used as the query and the rest of the
    mini-batch is used as the retrieval set. The positive set is formed of the other instances in the batch from the
    same class. The loss returns the average Smooth-AP across all instances in the mini-batch.
    Args:
        anneal : float
            the temperature of the sigmoid that is used to smooth the ranking function. A low value of the temperature
            results in a steep sigmoid, that tightly approximates the heaviside step function in the ranking function.
        num_id : int
            the number of different classes that are represented in the batch.
    Shape:
        - Input (preds): (batch_size, feat_dims) (must be a cuda torch float tensor)
        - Output: scalar
    """

    def __init__(self, anneal, num_id):
        super(SmoothAP, self).__init__()
        self.anneal = anneal
        self.num_id = num_id

    def forward(self, preds):
        """
        Forward pass for all input predictions: preds - (batch_size x feat_dims)
        """
        self.batch_size, self.feat_dims = preds.shape
        assert self.batch_size % self.num_id == 0

        # ------ differentiable ranking of all retrieval set ------
        # compute the mask which ignores the relevance score of the query to itself
        mask = 1.0 - torch.eye(self.batch_size)
        mask = mask.unsqueeze(dim=0).repeat(self.batch_size, 1, 1)
        # compute the relevance scores via cosine similarity of the CNN-produced embedding vectors
        sim_all = torch.mm(preds, preds.t())
        # compute the difference matrix
        # sim_all_repeat = sim_all.unsqueeze(dim=1).repeat(1, self.batch_size, 1)
        # sim_diff = sim_all_repeat - sim_all_repeat.permute(0, 2, 1)
        sim_diff = sim_all.unsqueeze(dim=1) - sim_all.unsqueeze(dim=2)  # shape: B x B x B
        # pass through the sigmoid
        sim_sg = sigmoid(sim_diff, temp=self.anneal) * mask.to(preds.device)
        # compute the rankings
        sim_all_rk = torch.sum(sim_sg, dim=-1) + 1

        # ------ differentiable ranking of only positive set in retrieval set ------
        # compute the mask which only gives non-zero weights to the positive set
        xs = preds.view(self.num_id, int(self.batch_size / self.num_id), self.feat_dims)
        pos_mask = 1.0 - torch.eye(int(self.batch_size / self.num_id))
        pos_mask = (
            pos_mask.unsqueeze(dim=0).unsqueeze(dim=0).repeat(self.num_id, int(self.batch_size / self.num_id), 1, 1)
        )
        # compute the relevance scores
        sim_pos = torch.bmm(xs, xs.permute(0, 2, 1))
        sim_pos_repeat = sim_pos.unsqueeze(dim=2).repeat(1, 1, int(self.batch_size / self.num_id), 1)
        # compute the difference matrix
        sim_pos_diff = sim_pos_repeat - sim_pos_repeat.permute(0, 1, 3, 2)
        # pass through the sigmoid
        sim_pos_sg = sigmoid(sim_pos_diff, temp=self.anneal) * pos_mask.to(preds.device)
        # compute the rankings of the positive set
        sim_pos_rk = torch.sum(sim_pos_sg, dim=-1) + 1

        # sum the values of the Smooth-AP for all instances in the mini-batch
        ap = torch.zeros(1).to(preds.device)
        group = int(self.batch_size / self.num_id)
        for ind in range(self.num_id):
            pos_divide = torch.sum(
                sim_pos_rk[ind] / (sim_all_rk[(ind * group) : ((ind + 1) * group), (ind * group) : ((ind + 1) * group)])
            )
            ap = ap + ((pos_divide / group) / self.batch_size)

        return 1 - ap


class MySmoothAP(nn.Module):
    """
    changed for random batch support
    """

    def __init__(self, anneal):
        super().__init__()
        self.anneal = anneal

    def forward(self, embeddings, labels):
        # compute the relevance scores via cosine similarity of the CNN-produced embedding vectors
        sim = embeddings @ embeddings.T
        # to filter positives
        target = labels @ labels.T > 0

        # compute the mask which ignores the relevance score of the query to itself
        mask = 1.0 - torch.eye(labels.size(0), device=labels.device).unsqueeze(dim=0)

        # compute the difference matrix: see HAPPIER
        D = sim.unsqueeze(dim=1) - sim.unsqueeze(dim=2)  # shape: B x B x B

        # pass through the sigmoid
        sg_all = sigmoid(D, temp=self.anneal) * mask

        # compute the rankings
        # shape: B x B
        rk_all = torch.sum(sg_all, dim=-1) + 1
        rk_all = rk_all[target]  # only pick query and rank_item is positive

        # calc s1~sm's rank of positive by using labels
        sg_pos = sg_all * target.unsqueeze(1)

        # compute the rankings of positive
        # shape: B x B
        rk_pos = torch.sum(sg_pos, dim=-1) + 1
        rk_pos = rk_pos[target]  # only pick query and rank_item is positive

        # calc mAP
        m_ap = (rk_pos / rk_all).mean()

        return 1 - m_ap


if __name__ == "__main__":
    batch_size = 9
    num_id = 3
    dim_embed = 5
    temp = 0.01

    # same data for SmoothAP & MySmoothAP
    loss1 = SmoothAP(temp, num_id)
    loss2 = MySmoothAP(temp)
    # embeddings = torch.randn(batch_size, dim_embed).cuda()
    embeddings = torch.nn.functional.normalize(torch.randn(batch_size, dim_embed))
    labels = torch.tensor(
        [
            [1, 0, 0],
            [1, 0, 0],
            [1, 0, 0],
            [0, 1, 0],
            [0, 1, 0],
            [0, 1, 0],
            [0, 0, 1],
            [0, 0, 1],
            [0, 0, 1],
        ]
    ).float()

    print(loss1(embeddings).item())
    print(loss2(embeddings, labels).item())
    # accuracy_calculator.py
    print(1 - mean_average_precision(embeddings, embeddings, labels, labels).item())
    print(1 - mean_average_precision(embeddings.sign(), embeddings.sign(), labels, labels).item())
